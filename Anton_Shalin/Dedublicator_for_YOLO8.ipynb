{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca75804c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "!!!!!!! Эта ячейка для запуска дедубликатора на Google Drive через Google Colab !!!!!!!\n",
    "\n",
    "'''\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "# Устанавливаем рабочую директорию\n",
    "pic_path = \"/content/drive/My Drive/UII/Capture\"\n",
    "# Создаем директорию для дубликатов\n",
    "dup_path = \"/content/drive/My Drive/UII/Capture/Duplicates\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c7eca1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "!!!!!!! Эта ячейка для запуска дедубликатора на локальной машине через Jupyter Notebook (или IDE) !!!!!!!\n",
    "\n",
    "'''\n",
    "\n",
    "# Устанавливаем рабочую директорию\n",
    "pic_path = r'C:\\Users\\anton\\UII\\AZavod_Ural'\n",
    "# Создаем директорию для дубликатов\n",
    "dup_path = r'C:\\Users\\anton\\UII\\AZavod_Ural\\Duplicates'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8b935e12",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Группировка по размеру: 100%|██████████| 1361/1361 [00:00<00:00, 3922.17 файлов/s]\n",
      "Обработка групп: 100%|██████████| 446/446 [00:11<00:00, 40.10 групп/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Похожие изображения успешно обработаны. Из 1361 файлов удалено 422 похожих и дубликатов. Осталось 939 разных изображений\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "!!!!!!! Собственно сам дедубликатор !!!!!!!\n",
    "\n",
    "'''\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial import KDTree\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "os.makedirs(dup_path, exist_ok=True)\n",
    "\n",
    "def dedublicator(similar_threshold, show_dups, images_group):\n",
    "    \n",
    "    # Функция для динамического вычисления размера хэша изображения, в зависимости от размера изображения\n",
    "    def dhash(image, base_size=64):\n",
    "        initial_hash_size = max(image.shape[:2]) / base_size # Вычисляем начальный размер хэша\n",
    "        hash_size = 2 ** math.ceil(math.log2(initial_hash_size)) # Округляем до ближайшей степени двойки\n",
    "        hash_size = max(4, hash_size) # Ограничиваем минимальный размер хэша\n",
    "        # Вычисляем хэш изображения\n",
    "        resized = cv2.resize(image, (hash_size + 1, hash_size))\n",
    "        diff = resized[:, 1:] > resized[:, :-1]\n",
    "        return diff.flatten().astype(int), hash_size  # Возвращаем хэш в виде вектора битов и размерность хэш-матрицы\n",
    "\n",
    "    # Функция для загрузки и хэширования изображений\n",
    "    def load_and_hash_image(path):\n",
    "        try:\n",
    "            # Загружаем изображение с помощью Pillow\n",
    "            with Image.open(path) as img:\n",
    "                # Конвертируем изображение в формат BGR для OpenCV\n",
    "                bgr_image = np.array(img.convert('RGB'))[:, :, ::-1]\n",
    "            # Применяем функцию dhash\n",
    "            dhash_value, _ = dhash(bgr_image)\n",
    "            return (path, (dhash_value, bgr_image))\n",
    "        except IOError:\n",
    "            print(f\"Не удалось загрузить изображение: {path}\")\n",
    "            return (path, (None, None))\n",
    "        \n",
    "    # Функция хэширования изображений в многопоточном режиме\n",
    "    def process_images(paths):\n",
    "        hash_dict = {}\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            results = list(executor.map(load_and_hash_image, paths))\n",
    "        for path, (dhash_value, original_image) in results:\n",
    "            if dhash_value is not None:\n",
    "                hash_dict[path] = (dhash_value, original_image)\n",
    "        return hash_dict\n",
    "\n",
    "    # Создаем KD-дерево для быстрого поиска близких хэшей\n",
    "    def create_kd_tree(hash_dict):\n",
    "        hash_values = [val[0] for val in hash_dict.values()]\n",
    "        tree = KDTree(hash_values)\n",
    "        return tree\n",
    "\n",
    "    # Функция для проверки схожести двух изображений\n",
    "    def is_similar(hash1, hash2, similar_threshold):\n",
    "        difference = np.sum(hash1 != hash2)\n",
    "        similarity = (1 - difference / len(hash1)) * 100\n",
    "        return similarity >= similar_threshold\n",
    "\n",
    "    # Сортировка по группам и поиск дубликатов по KD-дереву\n",
    "    def find_duplicates(hash_dict, kd_tree, similar_threshold):\n",
    "        duplicate_groups = {}\n",
    "        for file, (dhash_value, _) in hash_dict.items():\n",
    "            similar_indices = kd_tree.query_ball_point(dhash_value, similar_threshold)\n",
    "            duplicate_group = [file]\n",
    "            for idx in similar_indices:\n",
    "                similar_file = list(hash_dict.keys())[idx]\n",
    "                if file != similar_file and is_similar(dhash_value, hash_dict[similar_file][0], similar_threshold):\n",
    "                    duplicate_group.append(similar_file)\n",
    "            if len(duplicate_group) > 1:\n",
    "                duplicate_groups[len(duplicate_groups)] = list(set(duplicate_group))\n",
    "        return duplicate_groups\n",
    "\n",
    "    # Функция для поиска дубликатов между группами для их объединения по пересекающимся элементам\n",
    "    def merge_duplicate_groups(duplicate_groups):\n",
    "        merged_groups = []\n",
    "        for group in duplicate_groups.values():\n",
    "            # Ищем существующую группу, с которой есть пересечение\n",
    "            found = False\n",
    "            for merged in merged_groups:\n",
    "                if set(group).intersection(merged):\n",
    "                    merged.update(group)\n",
    "                    found = True\n",
    "                    break\n",
    "            if not found:\n",
    "                merged_groups.append(set(group))\n",
    "        return merged_groups\n",
    "\n",
    "    # Функция для отображения изображений\n",
    "    def display_images(images):\n",
    "        n_images = len(images)\n",
    "        plt.figure(figsize=(5 * n_images, 5))  # Изменяем размер фигуры в зависимости от количества изображений\n",
    "        for i, (title, image) in enumerate(images.items()):\n",
    "            ax = plt.subplot(1, n_images, i + 1)  # Создаем подграфик для каждого изображения\n",
    "            ax.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "            ax.axis('off')\n",
    "            ax.set_title(title)\n",
    "        plt.show()\n",
    "\n",
    "    # Основной код для обработки группы изображений\n",
    "    hash_dict = process_images(images_group) # Вычисление хэшей\n",
    "    if len(hash_dict) > 1:\n",
    "        kd_tree = create_kd_tree(hash_dict) # Создание KD-дерева\n",
    "        duplicate_groups = find_duplicates(hash_dict, kd_tree, similar_threshold) # Поиск дубликатов\n",
    "        merged_duplicate_groups = merge_duplicate_groups(duplicate_groups) # Объединение групп дубликатов\n",
    "\n",
    "        # Обработка и перемещение дубликатов\n",
    "        for group in merged_duplicate_groups:\n",
    "            if len(group) > 1: # Игнорируем группы с одним файлом\n",
    "                original = sorted(group)[len(group) // 2] # Берем медианный файл в группе как оригинал\n",
    "                duplicates = group - {original} # Остальные файлы считаем дубликатами\n",
    "                # Выводим дубликаты и оригинал на экран\n",
    "                if show_dups:\n",
    "                    images_to_show = {f\"Original\": hash_dict[original][1]}\n",
    "                    for i, dup in enumerate(duplicates):\n",
    "                        images_to_show[f\"Duplicate {i+1}\"] = hash_dict[dup][1]\n",
    "                    display_images(images_to_show)\n",
    "                # Перемещаем похожие файлы\n",
    "                for dup in duplicates:\n",
    "                    dup_filename = os.path.basename(dup)\n",
    "                    target_path = os.path.join(dup_path, dup_filename)\n",
    "                    if not os.path.exists(target_path):\n",
    "                        shutil.move(dup, target_path)\n",
    "\n",
    "# Функция для группировки изображений по размерам\n",
    "def group_images_by_size(directory):\n",
    "    size_groups = defaultdict(list)\n",
    "    for file in tqdm(os.listdir(directory), desc=\"Группировка по размеру\", unit=\" файлов\"):\n",
    "        if file.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp', '.tif', '.tiff')):\n",
    "            path = os.path.join(directory, file)\n",
    "            try:\n",
    "                image = Image.open(path)\n",
    "                size = image.size\n",
    "            except IOError:\n",
    "                print(f\"Failed to load image: {file}\")\n",
    "                continue\n",
    "            if image is not None:\n",
    "                size_groups[size].append(path)\n",
    "    return size_groups.values()\n",
    "\n",
    "#============================  Основное тело  ============================#\n",
    "\n",
    "# Параметры\n",
    "\n",
    "similar_threshold = 77 # степень похожести кадров (в %)\n",
    "show_dups=False # выводить ли на экран оригинал с дубликатами\n",
    "\n",
    "image_groups = group_images_by_size(pic_path)\n",
    "\n",
    "# Ищем и убираем дубликаты в каждой подгруппе по размерам изображений\n",
    "for group in tqdm(image_groups, desc=\"Обработка групп\", unit=\" групп\"):\n",
    "    dedublicator(similar_threshold, show_dups, group)\n",
    "\n",
    "cnt_img_org = len(os.listdir(pic_path))  # Подсчет количества изображений в исходной папке\n",
    "cnt_img_dup = len(os.listdir(dup_path)) # Подсчет количества перемещенных изображений\n",
    "print(f\"Похожие изображения успешно обработаны. Из {cnt_img_org+cnt_img_dup} файлов удалено {cnt_img_dup} похожих и дубликатов. Осталось {cnt_img_org} разных изображений\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
