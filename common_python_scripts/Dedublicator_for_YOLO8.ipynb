{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca75804c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "!!!!!!! Эта ячейка для запуска дедубликатора на Google Drive через Google Colab !!!!!!!\n",
    "\n",
    "'''\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "# Устанавливаем рабочую директорию\n",
    "pic_path = \"/content/drive/My Drive/UII/Capture\"\n",
    "# Создаем директорию для дубликатов\n",
    "dup_path = \"/content/drive/My Drive/UII/Capture/Duplicates\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c7eca1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "!!!!!!! Эта ячейка для запуска дедубликатора на локальной машине через Jupyter Notebook (или IDE) !!!!!!!\n",
    "\n",
    "'''\n",
    "\n",
    "# Устанавливаем рабочую директорию\n",
    "pic_path = r'C:\\Users\\anton\\UII\\AZavod_Ural'\n",
    "# Создаем директорию для дубликатов\n",
    "dup_path = r'C:\\Users\\anton\\UII\\AZavod_Ural\\Duplicates'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "002a2549",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Предобработка и хэширование: 100%|██████████| 1351/1351 [00:05<00:00, 244.67 файлов/s]\n",
      "Поиск дубликатов: 100%|██████████| 1351/1351 [01:20<00:00, 16.79 изображений/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Похожие изображения успешно обработаны. Из 1364 файлов удалено 289 похожих и дубликатов. Осталось 1075 разных изображений\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "!!!!!!! Собственно сам дедубликатор !!!!!!!\n",
    "\n",
    "'''\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import random\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial import KDTree\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import math\n",
    "import gc\n",
    "from collections import defaultdict\n",
    "import concurrent.futures\n",
    "\n",
    "os.makedirs(dup_path, exist_ok=True)\n",
    "   \n",
    "# Функция для вычисления структурного хэша изображения с динамической размерностью, в зависимости от размера изображения\n",
    "def dhash(image, base_size=64):\n",
    "    initial_hash_size = max(image.shape[:2]) / base_size # Вычисляем начальный размер хэша\n",
    "    hash_size = 2 ** math.ceil(math.log2(initial_hash_size)) # Округляем до ближайшей степени двойки\n",
    "    hash_size = max(4, hash_size) # Ограничиваем минимальный размер хэша\n",
    "    # Вычисляем хэш изображения\n",
    "    resized = cv2.resize(image, (hash_size + 1, hash_size))\n",
    "    diff = resized[:, 1:] > resized[:, :-1]\n",
    "    return diff.flatten().astype(int)  # Возвращаем хэш в виде вектора битов\n",
    "\n",
    "# Функция для вычисления цветового хэша изображения\n",
    "def color_hash(image, bins=8):\n",
    "    # Конвертируем изображение в HSV\n",
    "    hsv_img = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    # Вычисляем гистограмму для каждого канала HSV\n",
    "    hist_h = cv2.calcHist([hsv_img], [0], None, [bins], [0, 180])\n",
    "    hist_s = cv2.calcHist([hsv_img], [1], None, [bins], [0, 256])\n",
    "    hist_v = cv2.calcHist([hsv_img], [2], None, [bins], [0, 256])\n",
    "    # Нормализуем гистограмму\n",
    "    cv2.normalize(hist_h, hist_h)\n",
    "    cv2.normalize(hist_s, hist_s)\n",
    "    cv2.normalize(hist_v, hist_v)\n",
    "    # Объединяем гистограммы в один вектор\n",
    "    return np.concatenate((hist_h, hist_s, hist_v)).flatten()\n",
    "\n",
    "# Функция для сравнения двух цветовых хэшей\n",
    "def compare_color_hashes(hash1, hash2):\n",
    "    # Вычисляем Евклидово расстояние между двумя хэшами\n",
    "    distance = np.linalg.norm(hash1 - hash2)\n",
    "    return distance    \n",
    "\n",
    "# Функция для загрузки и хэширования изображений      \n",
    "def load_and_hash_image(path):\n",
    "    try:\n",
    "        # Загружаем изображение с помощью Pillow\n",
    "        with Image.open(path) as img:\n",
    "            # Конвертируем изображение в формат BGR для OpenCV                \n",
    "            bgr_image = np.array(img.convert('RGB'))[:, :, ::-1]\n",
    "        dhash_value = dhash(bgr_image)  # Вычисляем структурный хэш\n",
    "        color_hash_value = color_hash(bgr_image)  # Вычисляем цветовой хэш\n",
    "        return (path, (dhash_value, color_hash_value, bgr_image))\n",
    "    except IOError:\n",
    "        print(f\"Не удалось загрузить изображение: {path}\")\n",
    "        return (path, (None, None, None))        \n",
    "\n",
    "# Функция хэширования изображений в многопоточном режиме\n",
    "def process_images(paths):\n",
    "    hash_dict = {}\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        results = list(executor.map(load_and_hash_image, paths))\n",
    "    for path, (dhash_value, color_hash_value, original_image) in results:\n",
    "        if dhash_value is not None:\n",
    "            hash_dict[path] = (dhash_value, color_hash_value, original_image)\n",
    "    gc.collect() # очистка памяти\n",
    "    return hash_dict\n",
    "\n",
    "# Функция для определения медианного размера изображений\n",
    "def median_image_size(images):\n",
    "    sizes = [Image.open(img).size for img in images]\n",
    "    widths, heights = zip(*sizes)\n",
    "    median_width = sorted(widths)[len(widths) // 2]\n",
    "    median_height = sorted(heights)[len(heights) // 2]\n",
    "    median_size = (median_width, median_height)\n",
    "    return median_size\n",
    "\n",
    "# Функция для изменения размера изображения до медианного размера\n",
    "def resize_to_median(image, median_size):\n",
    "    return cv2.resize(image, median_size, interpolation=cv2.INTER_AREA)    \n",
    "\n",
    "# Функция хэширования изображений с приведенными размерами в многопоточном режиме\n",
    "def process_images_resize_mode(paths, median_size):\n",
    "    def process_single_image(path):\n",
    "        try:\n",
    "            with Image.open(path) as img:\n",
    "                bgr_image = np.array(img.convert('RGB'))[:, :, ::-1]\n",
    "                resized_image = resize_to_median(bgr_image, median_size)\n",
    "            dhash_value = dhash(resized_image)\n",
    "            color_hash_value = color_hash(resized_image)\n",
    "            return (path, (dhash_value, color_hash_value, bgr_image))\n",
    "        except IOError:\n",
    "            print(f\"Не удалось загрузить изображение: {path}\")\n",
    "            return (path, (None, None, None))\n",
    "\n",
    "    hash_dict = {}\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = [executor.submit(process_single_image, path) for path in paths]\n",
    "        for future in tqdm(concurrent.futures.as_completed(futures), total=len(paths), desc=\"Предобработка и хэширование\", unit=\" файлов\"):\n",
    "            path, data = future.result()\n",
    "            if data[0] is not None:\n",
    "                hash_dict[path] = data\n",
    "    gc.collect() # очистка памяти\n",
    "    return hash_dict\n",
    "\n",
    "# Создаем KD-дерево для быстрого поиска близких хэшей\n",
    "def create_kd_tree(hash_dict):\n",
    "    hash_values = [val[0] for val in hash_dict.values()]\n",
    "    tree = KDTree(hash_values)\n",
    "    return tree\n",
    "\n",
    "# Функция для проверки схожести двух изображений по структурному и цветовому хэшам\n",
    "def is_similar(hash1, hash2, color_hash1, color_hash2, similar_threshold):\n",
    "    structural_similarity = (1 - np.sum(hash1 != hash2) / len(hash1)) * 100\n",
    "    color_similarity = (1 - compare_color_hashes(color_hash1, color_hash2)) * 100\n",
    "    total_similarity = (structural_similarity + color_similarity) / 2\n",
    "    return total_similarity >= similar_threshold\n",
    "\n",
    "# Функция поиска дубликатов по KD-дереву  \n",
    "def find_duplicates(hash_dict, kd_tree, similar_threshold, show_progress):\n",
    "    duplicate_groups = {}\n",
    "    iterable = tqdm(enumerate(hash_dict.items()), total=len(hash_dict), desc=\"Поиск дубликатов\", unit=\" изображений\") if show_progress else enumerate(hash_dict.items())\n",
    "    for idx, (file, (dhash_value, color_hash_value, _)) in iterable:\n",
    "        similar_indices = kd_tree.query_ball_point(dhash_value, similar_threshold)\n",
    "        duplicate_group = [file]\n",
    "        for idx in similar_indices:\n",
    "            similar_file = list(hash_dict.keys())[idx]\n",
    "            if file != similar_file:\n",
    "                similar_dhash, similar_color_hash, _ = hash_dict[similar_file]\n",
    "                if is_similar(dhash_value, similar_dhash, color_hash_value, similar_color_hash, similar_threshold):\n",
    "                    duplicate_group.append(similar_file)\n",
    "        if len(duplicate_group) > 1:\n",
    "            duplicate_groups[len(duplicate_groups)] = list(set(duplicate_group))\n",
    "    return duplicate_groups\n",
    "\n",
    "# Функция для поиска дубликатов между группами для их объединения по пересекающимся элементам\n",
    "def merge_duplicate_groups(duplicate_groups):\n",
    "    merged_groups = []\n",
    "    for group in duplicate_groups.values():\n",
    "        # Ищем существующую группу, с которой есть пересечение\n",
    "        found = False\n",
    "        for merged in merged_groups:\n",
    "            if set(group).intersection(merged):\n",
    "                merged.update(group)\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "            merged_groups.append(set(group))\n",
    "    return merged_groups\n",
    "\n",
    "# Функция для отображения изображений\n",
    "def display_images(images):\n",
    "    n_images = len(images)\n",
    "    plt.figure(figsize=(5 * n_images, 5))  # Изменяем размер фигуры в зависимости от количества изображений\n",
    "    for i, (title, image) in enumerate(images.items()):\n",
    "        ax = plt.subplot(1, n_images, i + 1)  # Создаем подграфик для каждого изображения\n",
    "        ax.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "        ax.axis('off')\n",
    "        ax.set_title(title)\n",
    "    plt.show()\n",
    "\n",
    "# Функция для группировки изображений по размерам\n",
    "def group_images_by_size(directory):\n",
    "    size_groups = defaultdict(list)\n",
    "    for file in tqdm(os.listdir(directory), desc=\"Группировка по размеру\", unit=\" файлов\"):\n",
    "        if file.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp', '.tif', '.tiff')):\n",
    "            path = os.path.join(directory, file)\n",
    "            try:\n",
    "                image = Image.open(path)\n",
    "                size = image.size\n",
    "            except IOError:\n",
    "                print(f\"Failed to load image: {file}\")\n",
    "                continue\n",
    "            if image is not None:\n",
    "                size_groups[size].append(path)\n",
    "    return size_groups.values()\n",
    "\n",
    "# Функция для дедубликации и перемещения дубликатов\n",
    "def deduplicate_and_move(hash_dict, similar_threshold, show_dups, resize_mode):\n",
    "    if len(hash_dict) > 1:\n",
    "        kd_tree = create_kd_tree(hash_dict)\n",
    "        duplicate_groups = find_duplicates(hash_dict, kd_tree, similar_threshold, resize_mode)\n",
    "        merged_duplicate_groups = merge_duplicate_groups(duplicate_groups)\n",
    "\n",
    "        for group in merged_duplicate_groups:    \n",
    "            if len(group) > 1:\n",
    "                original = random.choice(list(group))\n",
    "                duplicates = group - {original}\n",
    "                if show_dups:\n",
    "                    images_to_show = {f\"Original\": hash_dict[original][2]}\n",
    "                    for i, dup in enumerate(duplicates):\n",
    "                        images_to_show[f\"Duplicate {i+1}\"] = hash_dict[dup][2]\n",
    "                    display_images(images_to_show)\n",
    "                for dup in duplicates:\n",
    "                    dup_filename = os.path.basename(dup)\n",
    "                    target_path = os.path.join(dup_path, dup_filename)\n",
    "                    if not os.path.exists(target_path):\n",
    "                        shutil.move(dup, target_path)\n",
    "\n",
    "# Основная функция для дедубликации с учетом режима ресайза\n",
    "def dedublicator(directory, similar_threshold, show_dups, resize_mode):\n",
    "    if resize_mode:\n",
    "        all_images = [os.path.join(directory, file) for file in os.listdir(directory) if file.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp', '.tif', '.tiff'))]\n",
    "        median_size = median_image_size(all_images)\n",
    "        hash_dict = process_images_resize_mode(all_images, median_size)\n",
    "        deduplicate_and_move(hash_dict, similar_threshold, show_dups, resize_mode)\n",
    "    else:\n",
    "        image_groups = group_images_by_size(directory)\n",
    "        for group in tqdm(image_groups, desc=\"Поиск дубликатов в группах\", unit=\" групп\"):\n",
    "            hash_dict = process_images(group)\n",
    "            deduplicate_and_move(hash_dict, similar_threshold, show_dups, resize_mode)                        \n",
    "\n",
    "#============================  Основное тело  ============================#\n",
    "\n",
    "# Параметры\n",
    "\n",
    "similar_threshold = 90 # степень похожести кадров (в %)\n",
    "show_dups = False # выводить ли на экран оригинал с дубликатами\n",
    "resize_mode = True  # Установите True для обработки любых размеров и False для поиска дубликатов в совпадающих размерах изображений\n",
    "   \n",
    "dedublicator(pic_path, similar_threshold, show_dups, resize_mode)\n",
    "gc.collect() # очистка памяти\n",
    "\n",
    "cnt_img_org = len(os.listdir(pic_path))  # Подсчет количества изображений в исходной папке\n",
    "cnt_img_dup = len(os.listdir(dup_path)) # Подсчет количества перемещенных изображений\n",
    "print(f\"Похожие изображения успешно обработаны. Из {cnt_img_org+cnt_img_dup} файлов удалено {cnt_img_dup} похожих и дубликатов. Осталось {cnt_img_org} разных изображений\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
